{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-10T18:19:10.936078Z","iopub.execute_input":"2023-03-10T18:19:10.936836Z","iopub.status.idle":"2023-03-10T18:19:10.957591Z","shell.execute_reply.started":"2023-03-10T18:19:10.936796Z","shell.execute_reply":"2023-03-10T18:19:10.956365Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/kaggle/input/news-category-dataset/News_Category_Dataset_v3.json\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom sklearn.metrics import classification_report\n","metadata":{"execution":{"iopub.status.busy":"2023-03-10T19:33:10.545588Z","iopub.execute_input":"2023-03-10T19:33:10.546808Z","iopub.status.idle":"2023-03-10T19:33:10.554147Z","shell.execute_reply.started":"2023-03-10T19:33:10.546754Z","shell.execute_reply":"2023-03-10T19:33:10.552870Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"df=pd.read_json(\"/kaggle/input/news-category-dataset/News_Category_Dataset_v3.json\",lines=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-10T18:19:28.096464Z","iopub.execute_input":"2023-03-10T18:19:28.097360Z","iopub.status.idle":"2023-03-10T18:19:29.774733Z","shell.execute_reply.started":"2023-03-10T18:19:28.097322Z","shell.execute_reply":"2023-03-10T18:19:29.773661Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Load the stopwords and initialize the stemmer and lemmatizer\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nstop_words = set(stopwords.words('english'))\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\ndef preprocess_text(text):\n    # Lowercase the text\n    text = text.lower()\n    # Remove punctuations\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    # Tokenize the text\n    tokens = word_tokenize(text)\n    # Remove stopwords and stem or lemmatize the words\n    words = [stemmer.stem(word) for word in tokens if word not in stop_words]\n    # words = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n    # Join the words back into a string\n    processed_text = ' '.join(words)\n    return processed_text","metadata":{"execution":{"iopub.status.busy":"2023-03-10T18:19:31.609785Z","iopub.execute_input":"2023-03-10T18:19:31.610677Z","iopub.status.idle":"2023-03-10T18:19:31.827793Z","shell.execute_reply.started":"2023-03-10T18:19:31.610637Z","shell.execute_reply":"2023-03-10T18:19:31.826596Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Preprocess the 'headline' column\ndf['headline'] = df['headline'].apply(preprocess_text)\n\n# Preprocess the 'short_description' column\ndf['short_description'] = df['short_description'].apply(preprocess_text)","metadata":{"execution":{"iopub.status.busy":"2023-03-10T18:19:36.602381Z","iopub.execute_input":"2023-03-10T18:19:36.602910Z","iopub.status.idle":"2023-03-10T18:22:15.831340Z","shell.execute_reply.started":"2023-03-10T18:19:36.602858Z","shell.execute_reply":"2023-03-10T18:22:15.830182Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"top_categories = ['POLITICS', 'WORLD NEWS', 'ENTERTAINMENT', 'SPORTS', 'BUSINESS', 'TECH']\n\n# Filter out rows with categories not in the top 6\ndf = df[df['category'].isin(top_categories)]\n","metadata":{"execution":{"iopub.status.busy":"2023-03-10T18:22:27.329729Z","iopub.execute_input":"2023-03-10T18:22:27.330797Z","iopub.status.idle":"2023-03-10T18:22:27.380832Z","shell.execute_reply.started":"2023-03-10T18:22:27.330744Z","shell.execute_reply":"2023-03-10T18:22:27.379734Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-10T18:22:49.325602Z","iopub.execute_input":"2023-03-10T18:22:49.326151Z","iopub.status.idle":"2023-03-10T18:22:49.335526Z","shell.execute_reply.started":"2023-03-10T18:22:49.326107Z","shell.execute_reply":"2023-03-10T18:22:49.334343Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"(69436, 6)"},"metadata":{}}]},{"cell_type":"code","source":"# Load the pre-trained BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Split the dataset into train and test sets\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(df['headline'] + ' ' + df['short_description'], df['category'], test_size=0.2, random_state=42)\n\n# Tokenize the train and test texts\ntrain_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True)\ntest_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-10T18:22:54.704144Z","iopub.execute_input":"2023-03-10T18:22:54.704584Z","iopub.status.idle":"2023-03-10T18:23:43.563587Z","shell.execute_reply.started":"2023-03-10T18:22:54.704539Z","shell.execute_reply":"2023-03-10T18:23:43.562500Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"185c74e62aff417cb66233f6e6d16ad6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea518811c854430cb112db92eb0fa250"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5e31618a2164600b50d904a10a52554"}},"metadata":{}}]},{"cell_type":"code","source":"# Convert the labels to numerical values\nlabels_map = {label: i for i, label in enumerate(train_labels.unique())}\ntrain_labels = [labels_map[label] for label in train_labels.tolist()]\ntest_labels = [labels_map[label] for label in test_labels.tolist()]\n\n# Convert the train and test data to PyTorch tensors\ntrain_dataset = torch.utils.data.TensorDataset(torch.tensor(train_encodings['input_ids']),\n                                               torch.tensor(train_encodings['attention_mask']),\n                                               torch.tensor(train_labels))\ntest_dataset = torch.utils.data.TensorDataset(torch.tensor(test_encodings['input_ids']),\n                                              torch.tensor(test_encodings['attention_mask']),\n                                              torch.tensor(test_labels))\n\n# Load the pre-trained BERT model and modify the last layer for classification\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(labels_map))\n","metadata":{"execution":{"iopub.status.busy":"2023-03-10T18:23:46.225342Z","iopub.execute_input":"2023-03-10T18:23:46.225972Z","iopub.status.idle":"2023-03-10T18:23:52.293240Z","shell.execute_reply.started":"2023-03-10T18:23:46.225933Z","shell.execute_reply":"2023-03-10T18:23:52.292165Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed5b558d1a9348aa8e755f5e2123744b"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\ninput_ids = input_ids.to(device)\nattention_mask = attention_mask.to(device)\nlabels = labels.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-03-10T18:26:09.680819Z","iopub.execute_input":"2023-03-10T18:26:09.681530Z","iopub.status.idle":"2023-03-10T18:26:09.810274Z","shell.execute_reply.started":"2023-03-10T18:26:09.681493Z","shell.execute_reply":"2023-03-10T18:26:09.809074Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Train the model\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\noptim = torch.optim.AdamW(model.parameters(), lr=5e-5)\nepochs = 3\nfor epoch in range(epochs):\n    for batch in train_loader:\n        optim.zero_grad()\n        input_ids = batch[0].to(device)\n        attention_mask = batch[1].to(device)\n        labels = batch[2].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs[0]\n        loss.backward()\n        optim.step()","metadata":{"execution":{"iopub.status.busy":"2023-03-10T18:26:11.763381Z","iopub.execute_input":"2023-03-10T18:26:11.764118Z","iopub.status.idle":"2023-03-10T19:20:58.058264Z","shell.execute_reply.started":"2023-03-10T18:26:11.764074Z","shell.execute_reply":"2023-03-10T19:20:58.056967Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model on the test set\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch[0].to(device)\n        attention_mask = batch[1].to(device)\n        labels = batch[2].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask)\n        _, predicted = torch.max(outputs[0], 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\naccuracy = correct / total\nprint('Test Accuracy: {:.2f}'.format(accuracy))","metadata":{"execution":{"iopub.status.busy":"2023-03-10T19:24:43.045331Z","iopub.execute_input":"2023-03-10T19:24:43.046196Z","iopub.status.idle":"2023-03-10T19:26:07.907131Z","shell.execute_reply.started":"2023-03-10T19:24:43.046158Z","shell.execute_reply":"2023-03-10T19:26:07.905767Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Test Accuracy: 0.88\n","output_type":"stream"}]},{"cell_type":"code","source":"# Get the predictions for the test set\nmodel.eval()\ny_pred = []\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch[0].to(device)\n        attention_mask = batch[1].to(device)\n        labels = batch[2].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask)\n        _, predicted = torch.max(outputs[0], 1)\n        y_pred.extend(predicted.cpu().numpy())\n\n# Get the actual labels for the test set\ny_true = test_labels\n\n# Print the classification report\ntarget_names = list(labels_map.keys())\nprint(classification_report(y_true, y_pred, target_names=target_names))","metadata":{"execution":{"iopub.status.busy":"2023-03-10T19:33:41.381195Z","iopub.execute_input":"2023-03-10T19:33:41.381567Z","iopub.status.idle":"2023-03-10T19:35:06.177644Z","shell.execute_reply.started":"2023-03-10T19:33:41.381532Z","shell.execute_reply":"2023-03-10T19:35:06.176172Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"               precision    recall  f1-score   support\n\n     POLITICS       0.93      0.92      0.92      7095\nENTERTAINMENT       0.90      0.92      0.91      3537\n       SPORTS       0.83      0.89      0.85       986\n     BUSINESS       0.75      0.68      0.72      1209\n   WORLD NEWS       0.71      0.79      0.75       642\n         TECH       0.75      0.65      0.70       419\n\n     accuracy                           0.88     13888\n    macro avg       0.81      0.81      0.81     13888\n weighted avg       0.88      0.88      0.88     13888\n\n","output_type":"stream"}]},{"cell_type":"code","source":"def predict_category(article_text):\n    # preprocess the article text\n    preprocessed_text = preprocess_text(article_text)\n    \n    # tokenize the text\n    input_ids = tokenizer.encode(preprocessed_text, truncation=True, padding=True, return_tensors='pt')\n    \n    # make a prediction\n    with torch.no_grad():\n        outputs = model(input_ids.to(device), attention_mask=None)\n        _, predicted = torch.max(outputs[0], 1)\n    \n    # get the predicted category label\n    label_map = {v: k for k, v in labels_map.items()}\n    predicted_label = label_map[predicted.item()]\n    \n    return predicted_label","metadata":{"execution":{"iopub.status.busy":"2023-03-10T19:38:23.513043Z","iopub.execute_input":"2023-03-10T19:38:23.514427Z","iopub.status.idle":"2023-03-10T19:38:23.521867Z","shell.execute_reply.started":"2023-03-10T19:38:23.514376Z","shell.execute_reply":"2023-03-10T19:38:23.520798Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"article = \"Apple unveils new iPhone\"\npredicted_category = predict_category(article)\nprint(predicted_category)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-10T20:14:46.769655Z","iopub.execute_input":"2023-03-10T20:14:46.770660Z","iopub.status.idle":"2023-03-10T20:14:46.793137Z","shell.execute_reply.started":"2023-03-10T20:14:46.770614Z","shell.execute_reply":"2023-03-10T20:14:46.792209Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"TECH\n","output_type":"stream"}]},{"cell_type":"code","source":"article = \"Donald Trump announces presidential campaign\"\npredicted_category = predict_category(article)\nprint(predicted_category)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-10T20:14:48.247650Z","iopub.execute_input":"2023-03-10T20:14:48.248795Z","iopub.status.idle":"2023-03-10T20:14:48.266869Z","shell.execute_reply.started":"2023-03-10T20:14:48.248733Z","shell.execute_reply":"2023-03-10T20:14:48.265745Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"POLITICS\n","output_type":"stream"}]},{"cell_type":"code","source":"article = \"Serena Williams wins Wimbledon\"\npredicted_category = predict_category(article)\nprint(predicted_category)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-10T20:14:49.315652Z","iopub.execute_input":"2023-03-10T20:14:49.316824Z","iopub.status.idle":"2023-03-10T20:14:49.335889Z","shell.execute_reply.started":"2023-03-10T20:14:49.316774Z","shell.execute_reply":"2023-03-10T20:14:49.334506Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"SPORTS\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}